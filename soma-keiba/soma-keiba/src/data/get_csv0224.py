# -*- coding: utf-8 -*-
"""netkeiba.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NIsiVB9mZkbNWWJ3-PO43Ojg5dzUQMKz
"""

import requests
from bs4 import BeautifulSoup
import numpy as np
import pandas as pd
import time
import re
import os

import shutil


def makeTableData(load_url):
    # Webページを取得して解析する
    html = requests.get(load_url)

    # リクエスト負荷回避
    time.sleep(1)
    soup = BeautifulSoup(html.content, "html.parser")

    table = soup.find(class_ = "race_table_01 nk_tb_common")
    tbody = table.find_all("tr")

    race_id = re.search("[0-9]+", load_url).group()
    table_2d = []
    for i in range(1, len(tbody)):
        table_1d = []
        line1 = tbody[i]
        line1_td = line1.find_all("td")
        for td in line1_td:
            table_1d.append(td.text.replace("\n", ""))
        if i <=3:
            table_1d.append(1)
        else:
            table_1d.append(0)
        table_1d.append(race_id) # race_idを追加
        table_2d.append(table_1d)
    result_df = pd.DataFrame(table_2d)
    result_df.columns = ["着順", "枠番", "馬番", "馬名", "性齢", "斤量","騎手", "タイム",
                        "着差", "ﾀｲﾑ指数", "通過","上り", "単勝", "人気", "馬体重",
                        "調教ﾀｲﾑ", "厩舎ｺﾒﾝﾄ", "備考", "調教師", "馬主", "賞金(万円)", "label","race_id"]
    result_df.drop(["ﾀｲﾑ指数", "調教ﾀｲﾑ", "厩舎ｺﾒﾝﾄ", "備考"], axis=1, inplace=True)

    # result_df["着順"] = result_df["着順"].astype(int)
    # result_df["label"] = result_df["着順"].apply(lambda x: 1 if x <= 3 else 0)
    return result_df

def appendToCsv(temp, file_path):
    df = pd.read_csv(file_path)
    # 元あるcsvに付け加える
    df = pd.concat([df, temp], axis=0)
    df.to_csv(file_path, index=False)

temp = pd.read_csv("./urls/2015-1.txt", header=None)
page_list = list(temp.iloc[:,0])

# how to use
# page_list = ["https://db.netkeiba.com/race/201905010201/",
#              "https://db.netkeiba.com/race/201905010202/",
#              "https://db.netkeiba.com/race/201905010203/"]

for page_url in page_list:
    temp = makeTableData(page_url)
    try:
        appendToCsv(temp, "df.csv")
    except:
        temp.to_csv("df.csv", index=False)

shutil.move('./df.csv', './csv_files/')

